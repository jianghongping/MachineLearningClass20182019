{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "190204WordVectors.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikp29/MachineLearningClass20182019/blob/master/Spring/190204WordVectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-qf2FC4OMXYW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## GPU Setup"
      ]
    },
    {
      "metadata": {
        "id": "1FZZooLaMfNR",
        "colab_type": "code",
        "outputId": "a968a01a-9cec-4131-f78f-de941038863a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cU53Jni9MkrN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "184da24f-4048-4d22-dddb-5d20a43b2f32"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "with tf.device('/cpu:0'):\n",
        "  random_image_cpu = tf.random_normal((100, 100, 100, 3))\n",
        "  net_cpu = tf.layers.conv2d(random_image_cpu, 32, 7)\n",
        "  net_cpu = tf.reduce_sum(net_cpu)\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  random_image_gpu = tf.random_normal((100, 100, 100, 3))\n",
        "  net_gpu = tf.layers.conv2d(random_image_gpu, 32, 7)\n",
        "  net_gpu = tf.reduce_sum(net_gpu)\n",
        "\n",
        "sess = tf.Session(config=config)\n",
        "\n",
        "# Test execution once to detect errors early.\n",
        "try:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "except tf.errors.InvalidArgumentError:\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise\n",
        "\n",
        "def cpu():\n",
        "  sess.run(net_cpu)\n",
        "  \n",
        "def gpu():\n",
        "  sess.run(net_gpu)\n",
        "  \n",
        "# Runs the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-055975cd90d7>:10: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "9.634823247\n",
            "GPU (s):\n",
            "1.8757040849999953\n",
            "GPU speedup over CPU: 5x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l_uWFD7aLfRs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n",
        "1. If you haven't already installed Python and Jupyter Notebook:   \n",
        "    1. Get Python3 from [Python.org](https://www.python.org/downloads/). **Tensorflow does not yet work with Python 3.7, so you _must_ get Python 3.6.** See https://github.com/tensorflow/tensorflow/issues/20517 for updates on 3.7 support.\n",
        "    1. In Terminal, run `python3 -m pip install jupyter`\n",
        "    1. In Terminal, cd to the folder in which you downloaded this file and run `jupyter notebook`. This should open up a page in your web browser that shows all of the files in the current directory, so that you can open this file. You will need to leave this Terminal window up and running and use a different one for the rest of the instructions.\n",
        "1. Install the Gensim word2vec Python implementation: `pip3 install --upgrade gensim`\n",
        "1. Get the trained model (1billion_word_vectors.zip) from me via airdrop or flashdrive and put it in the same folder as the ipynb file, the folder in which you are running the jupyter notebook command.\n",
        "1. Unzip the trained model file. You should now have three files in the folder (if zip created a new folder, move these files out of that separate folder into the same folder as the ipynb file):\n",
        "    * 1billion_word_vectors\n",
        "    * 1billion_word_vectors.syn1neg.npy\n",
        "    * 1billion_word_vectors.wv.syn0.npy\n",
        "1. If you didn't install keras last time, install it now\n",
        "    1. Install the tensorflow machine learning library by typing the following into Terminal:\n",
        "    `pip3 install --upgrade tensorflow`\n",
        "    1. Install the keras machine learning library by typing the following into Terminal:\n",
        "    `pip3 install keras`\n"
      ]
    },
    {
      "metadata": {
        "id": "pAfDnkQ3LfRv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Documentation/Sources\n",
        "* [https://radimrehurek.com/gensim/models/word2vec.html](https://radimrehurek.com/gensim/models/word2vec.html) for more information about how to use gensim word2vec in general\n",
        "* [https://codekansas.github.io/blog/2016/gensim.html](https://codekansas.github.io/blog/2016/gensim.html) for information about using it to create embedding layers for neural networks.\n",
        "* [https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/) for information on sequence classification with keras\n",
        "* [https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) for using pre-trained embeddings with keras (though the syntax they use for the model layers is different than most other tutorials I've seen).\n",
        "* [https://keras.io/](https://keras.io/) Keras API documentation"
      ]
    },
    {
      "metadata": {
        "id": "5iT29zQyLfRv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the trained word vectors"
      ]
    },
    {
      "metadata": {
        "id": "zYz2INNWLfRw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import word2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aplqMfB2LfS1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we have a way to turn words into word vectors with Keras layers. Yes! Time to get some data."
      ]
    },
    {
      "metadata": {
        "id": "ZPcUdBnGLfTm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Exercise: Use the word vectors in a full model\n",
        "Using the knowledge about how the imdb dataset and the keras embedding layer represent words, as detailed above, define a model that uses the pre-trained word vectors from the imdb dataset rather than an embedding that keras learns as it goes along. You'll need to swap out the embedding layer and feed in different training data.\n",
        "\n",
        "For any model that you try, take notes about the performance you see or anything you notice about the differences between each of them."
      ]
    },
    {
      "metadata": {
        "id": "D4MRWuCKLfTO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Process the dataset\n",
        "For this exercise, we're going to keep all inputs the same length (we'll see how to do variable-length later). This means we need to choose a maximum length for the review, cutting off longer ones and adding padding to shorter ones. What should we make the length? Let's understand our data."
      ]
    },
    {
      "metadata": {
        "id": "dxUgJfZTLfTm",
        "colab_type": "code",
        "outputId": "42d96197-7f9b-4a7f-f74d-990131aed460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AjTibju1Va1L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, Dense, Flatten, MaxPooling1D, Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3ZvNw1sXTuK6",
        "colab_type": "code",
        "outputId": "bdb98ace-f7f5-433d-e46b-fd31d14d83b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "imdb_offset = 3\n",
        "imdb_map = dict((index + imdb_offset, word) for (word, index) in imdb.get_word_index().items())\n",
        "imdb_map[0] = 'PADDING'\n",
        "imdb_map[1] = 'START'\n",
        "imdb_map[2] = 'UNKNOWN'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JErjNuS3v7rW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_sentences = [['PADDING'] + [imdb_map[word_index] for word_index in review] for review in x_train]\n",
        "test_sentences = [['PADDING'] + [imdb_map[word_index] for word_index in review] for review in x_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bvAljFGyLfTr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  imdb_wv_model = word2vec.Word2Vec(train_sentences + test_sentences + ['UNKNOWN'], min_count=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vBO4F9ZsLfTu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "imdb_wordvec = imdb_wv_model.wv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RrFNCQzwps5C",
        "colab_type": "code",
        "outputId": "9ff9fc71-95a6-4f05-b5f1-6bd7fe44705b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "imdb_wordvec.vocab['PADDING'].index"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "1LbtUnJgqrz1",
        "colab_type": "code",
        "outputId": "abfce62a-e416-49bc-882c-7dc3ffbee601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "map_to_wordvec = {} #structured like map index : wordvec index\n",
        "for map_index in imdb_map:\n",
        "  word = imdb_map[map_index]\n",
        "  if word == \"'l'\":\n",
        "    print(map_index)\n",
        "    wordvec_index=1841\n",
        "  else:\n",
        "    wordvec_index = imdb_wordvec.vocab[word].index\n",
        "  map_to_wordvec.update({str(map_index):wordvec_index})"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88587\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZC__GOjBLfTP",
        "colab_type": "code",
        "outputId": "48b034ff-976e-44e8-8fba-724abff42775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "lengths = [len(review) for review in x_train + x_test]\n",
        "print('Longest review: {} Shortest review: {}'.format(max(lengths), min(lengths)))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Longest review: 2697 Shortest review: 70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jLFmhH9KLfTS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2697 words! Wow. Well, let's see how many reviews would get cut off at a particular cutoff."
      ]
    },
    {
      "metadata": {
        "id": "Oy5cXxj7LfTU",
        "colab_type": "code",
        "outputId": "3521e79a-714a-4ef5-f05d-45222ca1f1db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cutoff = 1000\n",
        "print('{} reviews out of {} are over {}.'.format(\n",
        "    sum([1 for length in lengths if length > cutoff]), \n",
        "    len(lengths), \n",
        "    cutoff))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1097 reviews out of 25000 are over 1000.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Pjfs8To4tCh5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_mapped = [[map_to_wordvec[str(word_index)] for word_index in review] for review in x_train]\n",
        "x_test_mapped = [[map_to_wordvec[str(word_index)] for word_index in review] for review in x_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wnrJ511wLfTY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence\n",
        "x_train_padded = sequence.pad_sequences(x_train, maxlen=cutoff,value=28)\n",
        "x_test_padded = sequence.pad_sequences(x_test, maxlen=cutoff,value=28)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KQEUoyhVLfTw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_embedding_layer = imdb_wordvec.get_keras_embedding(train_embeddings=False)\n",
        "test_embedding_layer.input_length = cutoff"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DLX3mmbRLfTy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vector_model = Sequential()\n",
        "vector_model.add(test_embedding_layer)\n",
        "vector_model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
        "vector_model.add(MaxPooling1D(pool_size=3))\n",
        "vector_model.add(Dropout(.1))\n",
        "vector_model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
        "vector_model.add(MaxPooling1D(pool_size=3))\n",
        "vector_model.add(Dropout(.2))\n",
        "vector_model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
        "vector_model.add(MaxPooling1D(pool_size=5))\n",
        "vector_model.add(Dropout(.3))\n",
        "\n",
        "vector_model.add(Flatten())\n",
        "vector_model.add(Dense(units=128, activation='relu'))\n",
        "vector_model.add(Dropout(.5))\n",
        "vector_model.add(Dense(units=64, activation='relu'))\n",
        "vector_model.add(Dropout(.5))\n",
        "vector_model.add(Dense(units=32, activation='relu'))\n",
        "vector_model.add(Dense(units=1, activation='sigmoid')) # because at the end, we want one yes/no answer\n",
        "vector_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M5XfVmd5U5MI",
        "colab_type": "code",
        "outputId": "ecffa22f-6b44-40e7-a632-ed9f3e411afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1108
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  # Train our model\n",
        "  vector_model.fit(x_train_padded, y_train, epochs=30, batch_size=256, validation_data=(x_test_padded, y_test))\n",
        "\n",
        "  # Evaluate our model\n",
        "  score = vector_model.evaluate(x_test_padded, y_test, verbose=0)\n",
        "  print('Test loss:', score[0])\n",
        "  print('Test accuracy:', score[1])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/30\n",
            "25000/25000 [==============================] - 5s 208us/step - loss: 0.2353 - binary_accuracy: 0.9024 - val_loss: 0.4175 - val_binary_accuracy: 0.8058\n",
            "Epoch 2/30\n",
            "25000/25000 [==============================] - 5s 186us/step - loss: 0.2184 - binary_accuracy: 0.9122 - val_loss: 0.4209 - val_binary_accuracy: 0.8066\n",
            "Epoch 3/30\n",
            "25000/25000 [==============================] - 5s 186us/step - loss: 0.2108 - binary_accuracy: 0.9153 - val_loss: 0.4210 - val_binary_accuracy: 0.8067\n",
            "Epoch 4/30\n",
            "25000/25000 [==============================] - 5s 188us/step - loss: 0.2089 - binary_accuracy: 0.9133 - val_loss: 0.4212 - val_binary_accuracy: 0.8059\n",
            "Epoch 5/30\n",
            "25000/25000 [==============================] - 5s 185us/step - loss: 0.2045 - binary_accuracy: 0.9148 - val_loss: 0.4313 - val_binary_accuracy: 0.8024\n",
            "Epoch 6/30\n",
            "25000/25000 [==============================] - 5s 186us/step - loss: 0.2024 - binary_accuracy: 0.9166 - val_loss: 0.4217 - val_binary_accuracy: 0.8066\n",
            "Epoch 7/30\n",
            "25000/25000 [==============================] - 5s 185us/step - loss: 0.2011 - binary_accuracy: 0.9185 - val_loss: 0.4237 - val_binary_accuracy: 0.8040\n",
            "Epoch 8/30\n",
            "25000/25000 [==============================] - 5s 185us/step - loss: 0.2002 - binary_accuracy: 0.9184 - val_loss: 0.4354 - val_binary_accuracy: 0.8016\n",
            "Epoch 9/30\n",
            "25000/25000 [==============================] - 5s 185us/step - loss: 0.1936 - binary_accuracy: 0.9197 - val_loss: 0.4250 - val_binary_accuracy: 0.8049\n",
            "Epoch 10/30\n",
            "25000/25000 [==============================] - 5s 187us/step - loss: 0.1928 - binary_accuracy: 0.9216 - val_loss: 0.4337 - val_binary_accuracy: 0.8040\n",
            "Epoch 11/30\n",
            "25000/25000 [==============================] - 5s 186us/step - loss: 0.1928 - binary_accuracy: 0.9198 - val_loss: 0.4332 - val_binary_accuracy: 0.8034\n",
            "Epoch 12/30\n",
            "25000/25000 [==============================] - 5s 187us/step - loss: 0.1915 - binary_accuracy: 0.9217 - val_loss: 0.4302 - val_binary_accuracy: 0.8035\n",
            "Epoch 13/30\n",
            "25000/25000 [==============================] - 5s 187us/step - loss: 0.1932 - binary_accuracy: 0.9190 - val_loss: 0.4375 - val_binary_accuracy: 0.7952\n",
            "Epoch 14/30\n",
            "25000/25000 [==============================] - 5s 188us/step - loss: 0.1935 - binary_accuracy: 0.9220 - val_loss: 0.4370 - val_binary_accuracy: 0.7992\n",
            "Epoch 15/30\n",
            "25000/25000 [==============================] - 5s 184us/step - loss: 0.1914 - binary_accuracy: 0.9224 - val_loss: 0.4308 - val_binary_accuracy: 0.8013\n",
            "Epoch 16/30\n",
            "25000/25000 [==============================] - 5s 184us/step - loss: 0.1872 - binary_accuracy: 0.9235 - val_loss: 0.4331 - val_binary_accuracy: 0.8035\n",
            "Epoch 17/30\n",
            "25000/25000 [==============================] - 5s 184us/step - loss: 0.1900 - binary_accuracy: 0.9202 - val_loss: 0.4573 - val_binary_accuracy: 0.7938\n",
            "Epoch 18/30\n",
            "25000/25000 [==============================] - 5s 184us/step - loss: 0.1862 - binary_accuracy: 0.9246 - val_loss: 0.4292 - val_binary_accuracy: 0.8028\n",
            "Epoch 19/30\n",
            "25000/25000 [==============================] - 5s 184us/step - loss: 0.1847 - binary_accuracy: 0.9243 - val_loss: 0.4716 - val_binary_accuracy: 0.7894\n",
            "Epoch 20/30\n",
            "25000/25000 [==============================] - 5s 184us/step - loss: 0.1926 - binary_accuracy: 0.9189 - val_loss: 0.4287 - val_binary_accuracy: 0.8048\n",
            "Epoch 21/30\n",
            "25000/25000 [==============================] - 5s 185us/step - loss: 0.1901 - binary_accuracy: 0.9224 - val_loss: 0.4306 - val_binary_accuracy: 0.8036\n",
            "Epoch 22/30\n",
            "25000/25000 [==============================] - 5s 189us/step - loss: 0.1831 - binary_accuracy: 0.9268 - val_loss: 0.4504 - val_binary_accuracy: 0.7904\n",
            "Epoch 23/30\n",
            "25000/25000 [==============================] - 5s 187us/step - loss: 0.1831 - binary_accuracy: 0.9248 - val_loss: 0.4315 - val_binary_accuracy: 0.8012\n",
            "Epoch 24/30\n",
            "25000/25000 [==============================] - 5s 187us/step - loss: 0.1808 - binary_accuracy: 0.9263 - val_loss: 0.4404 - val_binary_accuracy: 0.7989\n",
            "Epoch 25/30\n",
            "25000/25000 [==============================] - 5s 194us/step - loss: 0.1813 - binary_accuracy: 0.9272 - val_loss: 0.4299 - val_binary_accuracy: 0.8055\n",
            "Epoch 26/30\n",
            "25000/25000 [==============================] - 5s 192us/step - loss: 0.1836 - binary_accuracy: 0.9228 - val_loss: 0.4335 - val_binary_accuracy: 0.8034\n",
            "Epoch 27/30\n",
            "25000/25000 [==============================] - 5s 198us/step - loss: 0.1833 - binary_accuracy: 0.9259 - val_loss: 0.4712 - val_binary_accuracy: 0.7866\n",
            "Epoch 28/30\n",
            "25000/25000 [==============================] - 5s 192us/step - loss: 0.1809 - binary_accuracy: 0.9262 - val_loss: 0.4492 - val_binary_accuracy: 0.7953\n",
            "Epoch 29/30\n",
            "25000/25000 [==============================] - 5s 185us/step - loss: 0.1883 - binary_accuracy: 0.9224 - val_loss: 0.4823 - val_binary_accuracy: 0.7793\n",
            "Epoch 30/30\n",
            "25000/25000 [==============================] - 5s 185us/step - loss: 0.1829 - binary_accuracy: 0.9234 - val_loss: 0.4270 - val_binary_accuracy: 0.8030\n",
            "Test loss: 0.42696055778503417\n",
            "Test accuracy: 0.803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "egsxjgSstXha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Notes\n",
        "\n",
        "The most optimized model I could create still only had ~ 80% accuracy. Anext step for me would be to use the vectors from the billion word vectors file and train my model with that layer."
      ]
    }
  ]
}